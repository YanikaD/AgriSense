{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "067057c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Folder of PDFs -> one TXT per PDF (Thai/English)\n",
    "- Recursively finds all *.pdf in input_dir\n",
    "- Tries digital text first (fast, accurate)\n",
    "- Falls back to EasyOCR (memory-safe: streams pages, caps size, tiles if needed)\n",
    "- Writes ONE combined <pdf_stem>_full.txt next to each PDF\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "# ====== CONFIG (edit as needed) ==============================================\n",
    "input_dir_str = r\"../../resource\"  # <--- change to your folder\n",
    "LANGS = ['th', 'en']               # OCR languages\n",
    "USE_GPU = False                    # Set True if you have CUDA\n",
    "MAX_DIM = 1600                     # Longest side for rendered page (lower if OOM)\n",
    "CANVAS_SIZE = 1920                 # EasyOCR detector canvas; try 1536 if RAM is tight\n",
    "TILE_H = 900                       # Tile height for OCR fallback\n",
    "TILE_OVERLAP = 80                  # Overlap between tiles\n",
    "DIGITAL_TEXT_MINLEN = 200          # If digital text len >= this, skip OCR\n",
    "ENCODING = \"utf-8-sig\"             # Helps Thai display in Windows Notepad\n",
    "SKIP_EXISTING = True               # Skip PDFs if corresponding _full.txt already exists\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    print(\"Please: pip install pymupdf\", file=sys.stderr)\n",
    "    raise\n",
    "\n",
    "def extract_digital_text(pdf_path: Path) -> str:\n",
    "    \"\"\"Extracts text without OCR. Returns concatenated text for all pages.\"\"\"\n",
    "    out = []\n",
    "    with fitz.open(str(pdf_path)) as doc:\n",
    "        for p in doc:\n",
    "            txt = p.get_text(\"text\")\n",
    "            out.append((txt or \"\").strip())\n",
    "    return \"\\n\\n==== PAGE BREAK ====\\n\\n\".join(out).strip()\n",
    "\n",
    "def iter_pdf_images(pdf_path: Path, max_dim: int = 1600, min_zoom: float = 1.0, max_zoom: float = 2.0):\n",
    "    \"\"\"Yield each page as a numpy RGB image, scaled so longest side <= max_dim.\"\"\"\n",
    "    import numpy as np\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    try:\n",
    "        for page in doc:\n",
    "            w, h = page.rect.width, page.rect.height\n",
    "            base_max = max(w, h)\n",
    "            zoom = max(min_zoom, min(max_zoom, max_dim / base_max))\n",
    "            mat = fitz.Matrix(zoom, zoom)\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "            img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)\n",
    "            yield img\n",
    "            del img\n",
    "            gc.collect()\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "def ocr_with_tiling(reader, img, tile_h: int = 1100, overlap: int = 80, **ocr_kwargs) -> str:\n",
    "    \"\"\"OCR a tall image in vertical tiles to reduce peak memory.\"\"\"\n",
    "    H, _W = img.shape[:2]\n",
    "    y = 0\n",
    "    texts = []\n",
    "    while y < H:\n",
    "        y0 = max(0, y - overlap)\n",
    "        y1 = min(H, y + tile_h)\n",
    "        tile = img[y0:y1, :, :]\n",
    "        lines = reader.readtext(tile, detail=0, paragraph=True, **ocr_kwargs)\n",
    "        texts.append(\"\\n\".join(lines))\n",
    "        y += tile_h - overlap\n",
    "        del tile\n",
    "        gc.collect()\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "def ocr_pdf_to_text(pdf_path: Path) -> str:\n",
    "    \"\"\"Digital-first; OCR fallback (streaming, capped size, tiling). Returns full combined text.\"\"\"\n",
    "    # 1) Try digital text layer\n",
    "    digital = extract_digital_text(pdf_path)\n",
    "    if len(digital) >= DIGITAL_TEXT_MINLEN:\n",
    "        print(f\"[INFO] {pdf_path.name}: digital text detected -> no OCR\")\n",
    "        return digital\n",
    "\n",
    "    print(f\"[INFO] {pdf_path.name}: little/no digital text -> OCR\")\n",
    "    # 2) OCR fallback\n",
    "    try:\n",
    "        import easyocr\n",
    "    except ImportError:\n",
    "        print(\"Please: pip install easyocr\", file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "    reader = easyocr.Reader(LANGS, gpu=USE_GPU)\n",
    "    OCR_KW = dict(canvas_size=CANVAS_SIZE, mag_ratio=1.0, batch_size=1, workers=0)\n",
    "\n",
    "    page_texts = []\n",
    "    for img in iter_pdf_images(pdf_path, max_dim=MAX_DIM):\n",
    "        try:\n",
    "            lines = reader.readtext(img, detail=0, paragraph=True, **OCR_KW)\n",
    "            page_texts.append(\"\\n\".join(lines))\n",
    "        except RuntimeError as e:\n",
    "            if \"DefaultCPUAllocator: not enough memory\" in str(e):\n",
    "                print(f\"[WARN] {pdf_path.name}: OOM on full page -> tiling\")\n",
    "                txt = ocr_with_tiling(reader, img, tile_h=TILE_H, overlap=TILE_OVERLAP, **OCR_KW)\n",
    "                page_texts.append(txt)\n",
    "            else:\n",
    "                raise\n",
    "        del img\n",
    "        gc.collect()\n",
    "\n",
    "    return \"\\n\\n==== PAGE BREAK ====\\n\\n\".join(page_texts).strip()\n",
    "\n",
    "def process_pdf(pdf_path: Path) -> Path:\n",
    "    \"\"\"Process one PDF and write <stem>_full.txt next to it. Returns txt path.\"\"\"\n",
    "    out_txt = pdf_path.with_suffix(\"\").with_name(pdf_path.stem + \"_full.txt\")\n",
    "    if SKIP_EXISTING and out_txt.exists():\n",
    "        print(f\"[SKIP] Already exists: {out_txt.name}\")\n",
    "        return out_txt\n",
    "\n",
    "    full_text = ocr_pdf_to_text(pdf_path)\n",
    "    with open(out_txt, \"w\", encoding=ENCODING, newline=\"\\n\") as f:\n",
    "        f.write(full_text or \"\")\n",
    "    print(f\"[DONE] {pdf_path.name} -> {out_txt.name}\")\n",
    "    return out_txt\n",
    "\n",
    "def main():\n",
    "    in_dir = Path(input_dir_str)\n",
    "    if not in_dir.exists():\n",
    "        raise FileNotFoundError(f\"Input folder not found: {in_dir.resolve()}\")\n",
    "\n",
    "    pdfs = sorted(in_dir.rglob(\"*.pdf\"))\n",
    "    if not pdfs:\n",
    "        print(f\"[INFO] No PDFs found under: {in_dir.resolve()}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Found {len(pdfs)} PDF(s) under {in_dir.resolve()}\\n\")\n",
    "    success, failed = 0, 0\n",
    "    for i, pdf_path in enumerate(pdfs, start=1):\n",
    "        try:\n",
    "            print(f\"[{i}/{len(pdfs)}] Processing: {pdf_path}\")\n",
    "            process_pdf(pdf_path)\n",
    "            success += 1\n",
    "        except Exception:\n",
    "            failed += 1\n",
    "            print(f\"[ERROR] Failed: {pdf_path.name}\")\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"\\n[SUMMARY] Success: {success}  Failed: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dec0648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 10 PDF(s) under D:\\Users\\Dell\\Documents\\GeoSpark\\rag-app\\backend\\resource\n",
      "\n",
      "[1/10] Processing: ..\\..\\resource\\107b991c3b5e1bdc.pdf\n",
      "[SKIP] Already exists: 107b991c3b5e1bdc_full.txt\n",
      "[2/10] Processing: ..\\..\\resource\\การปรับตัวรองรับการเปลี่ยนแปลงสภาพภูมิอากาศของเกษตรกรชาวสวนยางพาราจังหวัดระยอง.pdf\n",
      "[SKIP] Already exists: การปรับตัวรองรับการเปลี่ยนแปลงสภาพภูมิอากาศของเกษตรกรชาวสวนยางพาราจังหวัดระยอง_full.txt\n",
      "[3/10] Processing: ..\\..\\resource\\ปัจจัยที่มีอิทธิพลต่อการปรับตัวของเกษตรกรชาวสวนยางพาราต่อการเปลี่ยนแปลงสภาพภูมิอากาสในพื้นที่ภาคใต้ตอนล่างของประเทศไทย.pdf\n",
      "[SKIP] Already exists: ปัจจัยที่มีอิทธิพลต่อการปรับตัวของเกษตรกรชาวสวนยางพาราต่อการเปลี่ยนแปลงสภาพภูมิอากาสในพื้นที่ภาคใต้ตอนล่างของประเทศไทย_full.txt\n",
      "[4/10] Processing: ..\\..\\resource\\ผลกระทบของการเปลี่ยนแปลงสภาพภูมิอากาศต่อผลผลิตยางพาราในพื้นที่เขตภาคใต้ตอนล่างของประเทศไทย.pdf\n",
      "[SKIP] Already exists: ผลกระทบของการเปลี่ยนแปลงสภาพภูมิอากาศต่อผลผลิตยางพาราในพื้นที่เขตภาคใต้ตอนล่างของประเทศไทย_full.txt\n",
      "[5/10] Processing: ..\\..\\resource\\มาตรการชะลอการเก็บเกี่ยวมันสำปะหลังปี2566_67.pdf\n",
      "[SKIP] Already exists: มาตรการชะลอการเก็บเกี่ยวมันสำปะหลังปี2566_67_full.txt\n",
      "[6/10] Processing: ..\\..\\resource\\มาตรการรักษาเสถียรภาพราคามันสำปะหลังปี2567_2568.pdf\n",
      "[SKIP] Already exists: มาตรการรักษาเสถียรภาพราคามันสำปะหลังปี2567_2568_full.txt\n",
      "[7/10] Processing: ..\\..\\resource\\รายงานการเปลี่ยนแปลงสภาพภูมิอากาศ.pdf\n",
      "[INFO] รายงานการเปลี่ยนแปลงสภาพภูมิอากาศ.pdf: digital text detected -> no OCR\n",
      "[DONE] รายงานการเปลี่ยนแปลงสภาพภูมิอากาศ.pdf -> รายงานการเปลี่ยนแปลงสภาพภูมิอากาศ_full.txt\n",
      "[8/10] Processing: ..\\..\\resource\\สถานการณ์ยางพฤษภาคม2568.pdf\n",
      "[SKIP] Already exists: สถานการณ์ยางพฤษภาคม2568_full.txt\n",
      "[9/10] Processing: ..\\..\\resource\\แผนปฏิบัติการประจําปีงบประมาณปี2567ของการยางแห่งประเทศไทย.pdf\n",
      "[SKIP] Already exists: แผนปฏิบัติการประจําปีงบประมาณปี2567ของการยางแห่งประเทศไทย_full.txt\n",
      "[10/10] Processing: ..\\..\\resource\\แผนวิสาหกิจการยางแห่งประเทศไทยพศ2566_2570.pdf\n",
      "[SKIP] Already exists: แผนวิสาหกิจการยางแห่งประเทศไทยพศ2566_2570_full.txt\n",
      "\n",
      "[SUMMARY] Success: 10  Failed: 0\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25190475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "PDFs -> Markdown (.md) with table handling (Thai/English)\n",
    "- Recursively finds all *.pdf in input_dir\n",
    "- DIGITAL-FIRST to Markdown via PyMuPDF4LLM (handles headings/lists/tables well)\n",
    "- If PyMuPDF4LLM unavailable or output is too short, fallback:\n",
    "  - pdfplumber text + table extraction -> reconstruct Markdown tables\n",
    "- Final fallback (scanned PDFs): EasyOCR (streamed, tiled). (Tables degrade to plain text)\n",
    "- Writes ONE <pdf_stem>.md next to each PDF\n",
    "\n",
    "Install (pick what you need):\n",
    "    pip install pymupdf pymupdf4llm pdfplumber easyocr pypdf pillow numpy\n",
    "\n",
    "Note: To respect “no long sentences in tables”, long cells are truncated\n",
    "and their full text is listed under the table as “Notes”.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import sys\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "input_dir_str = r\"../../resource\"   # <--- change to your folder\n",
    "ENCODING = \"utf-8-sig\"              # Windows-friendly for Thai\n",
    "SKIP_EXISTING = True                # Skip PDFs if *.md already exists\n",
    "\n",
    "# Thresholds / behavior\n",
    "DIGITAL_MD_MINLEN = 300             # if digital markdown len >= this, accept\n",
    "TRUNCATE_TABLE_CELL_CHARS = 120     # avoid long sentences in MD tables\n",
    "REPLACE_VERTICAL_BAR = \"¦\"          # replace '|' in cells to avoid MD break\n",
    "\n",
    "# OCR fallback (for scanned PDFs)\n",
    "LANGS = ['th', 'en']                # EasyOCR languages\n",
    "USE_GPU = False                     # True if you have CUDA\n",
    "MAX_DIM = 1600                      # Longest side when rasterizing pages\n",
    "CANVAS_SIZE = 1920                  # EasyOCR detector canvas\n",
    "TILE_H = 900                        # Tile height for OCR\n",
    "TILE_OVERLAP = 80                   # Overlap between tiles\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "# ----------------- Utilities -----------------\n",
    "def normalize_whitespace(s: str) -> str:\n",
    "    return re.sub(r\"[ \\t]+\", \" \", (s or \"\").replace(\"\\r\", \"\")).strip()\n",
    "\n",
    "def safe_cell(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).replace(\"|\", REPLACE_VERTICAL_BAR)\n",
    "    s = s.replace(\"\\n\", \"<br>\")\n",
    "    return s.strip()\n",
    "\n",
    "def make_md_table(rows):\n",
    "    \"\"\"\n",
    "    Build a Markdown table from list-of-rows (strings).\n",
    "    Enforces short cells; long cells get footnotes under the table.\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        return \"\", 0\n",
    "\n",
    "    # Normalize rows to strings\n",
    "    rows = [[safe_cell(c) for c in (r or [])] for r in rows]\n",
    "    # Make all rows same length\n",
    "    max_cols = max((len(r) for r in rows), default=0)\n",
    "    rows = [r + [\"\"] * (max_cols - len(r)) for r in rows]\n",
    "\n",
    "    # Use first row as header; if it's mostly numeric/blank, synthesize headers\n",
    "    header = rows[0] if rows else []\n",
    "    if sum(1 for c in header if c and not c.replace(\".\", \"\", 1).isdigit()) < max(1, max_cols // 2):\n",
    "        header = [f\"Col{i+1}\" for i in range(max_cols)]\n",
    "        body = rows\n",
    "    else:\n",
    "        body = rows[1:]\n",
    "\n",
    "    # Truncate long cells and collect notes\n",
    "    notes = []\n",
    "    def maybe_trunc(c, r_idx, c_idx):\n",
    "        if len(c) > TRUNCATE_TABLE_CELL_CHARS:\n",
    "            note_idx = len(notes) + 1\n",
    "            notes.append((note_idx, r_idx, c_idx, c))\n",
    "            return f\"…[{note_idx}]\"\n",
    "        return c\n",
    "\n",
    "    header = [maybe_trunc(c, 0, j) for j, c in enumerate(header)]\n",
    "    new_body = []\n",
    "    for i, r in enumerate(body, start=1):\n",
    "        new_body.append([maybe_trunc(c, i, j) for j, c in enumerate(r)])\n",
    "    body = new_body\n",
    "\n",
    "    # Build MD\n",
    "    md = []\n",
    "    md.append(\"| \" + \" | \".join(header) + \" |\")\n",
    "    md.append(\"| \" + \" | \".join([\"---\"] * max_cols) + \" |\")\n",
    "    for r in body:\n",
    "        md.append(\"| \" + \" | \".join(r) + \" |\")\n",
    "\n",
    "    if notes:\n",
    "        md.append(\"\")\n",
    "        md.append(\"_Notes_:\")\n",
    "        for idx, r_i, c_i, txt in notes:\n",
    "            md.append(f\"- [{idx}] R{r_i+1}C{c_i+1}: {txt}\")\n",
    "\n",
    "    return \"\\n\".join(md), len(notes)\n",
    "\n",
    "\n",
    "# ----------------- DIGITAL FIRST: PyMuPDF4LLM -----------------\n",
    "def digital_md_pymupdf4llm(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        import pymupdf4llm  # type: ignore\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # Fast, layout-aware Markdown (handles tables / bullets / headers)\n",
    "        md = pymupdf4llm.to_markdown(\n",
    "            str(pdf_path),\n",
    "            page_chunks=False,\n",
    "            write_images=False,   # no images in MD\n",
    "            ocr=False             # we handle OCR fallback separately\n",
    "        )\n",
    "        return (md or \"\").strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# ----------------- DIGITAL FALLBACK: pdfplumber -----------------\n",
    "def digital_md_pdfplumber(pdf_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Page-by-page:\n",
    "      - extract tables (multiple strategies)\n",
    "      - extract page text\n",
    "      - compose Markdown with tables + text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import pdfplumber  # type: ignore\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "    parts = []\n",
    "    try:\n",
    "        with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "            for pi, page in enumerate(pdf.pages, start=1):\n",
    "                page_lines = []\n",
    "\n",
    "                # Try multi-strategy table detection\n",
    "                table_settings_list = [\n",
    "                    {\"vertical_strategy\": \"lines\", \"horizontal_strategy\": \"lines\"},\n",
    "                    {\"vertical_strategy\": \"text\", \"horizontal_strategy\": \"text\"},\n",
    "                    {\"vertical_strategy\": \"lines\", \"horizontal_strategy\": \"text\"},\n",
    "                    {\"vertical_strategy\": \"text\", \"horizontal_strategy\": \"lines\"},\n",
    "                ]\n",
    "\n",
    "                found_table = False\n",
    "                used_tables = []\n",
    "\n",
    "                for ts in table_settings_list:\n",
    "                    try:\n",
    "                        tables = page.extract_tables(table_settings=ts) or []\n",
    "                        # Filter out single-cell artifacts\n",
    "                        tables = [t for t in tables if t and any(any(c for c in row) for row in t)]\n",
    "                        if tables:\n",
    "                            used_tables = tables\n",
    "                            found_table = True\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                if found_table:\n",
    "                    page_lines.append(f\"## Page {pi} — Tables\")\n",
    "                    for ti, tbl in enumerate(used_tables, start=1):\n",
    "                        # Normalize rows and drop totally empty rows\n",
    "                        norm_rows = []\n",
    "                        for row in tbl:\n",
    "                            row = [normalize_whitespace(c) for c in (row or [])]\n",
    "                            if any(row):\n",
    "                                norm_rows.append(row)\n",
    "                        if not norm_rows:\n",
    "                            continue\n",
    "\n",
    "                        md_table, _notes = make_md_table(norm_rows)\n",
    "                        page_lines.append(f\"**Table {pi}.{ti}**\")\n",
    "                        page_lines.append(md_table)\n",
    "                        page_lines.append(\"\")\n",
    "\n",
    "                # Page text (avoid dupe by not trying to subtract table areas)\n",
    "                txt = page.extract_text() or \"\"\n",
    "                txt = txt.strip()\n",
    "                if txt:\n",
    "                    page_lines.append(f\"## Page {pi} — Text\")\n",
    "                    page_lines.append(txt)\n",
    "\n",
    "                if page_lines:\n",
    "                    parts.append(\"\\n\\n\".join(page_lines))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "    return (\"\\n\\n----\\n\\n\".join(parts)).strip()\n",
    "\n",
    "\n",
    "# ----------------- SCANNED FALLBACK: EasyOCR -----------------\n",
    "def iter_pdf_images(pdf_path: Path, max_dim: int = 1600, min_zoom: float = 1.0, max_zoom: float = 2.0):\n",
    "    \"\"\"Yield each page as a numpy RGB image, scaled so longest side <= max_dim.\"\"\"\n",
    "    import numpy as np\n",
    "    import fitz  # PyMuPDF\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    try:\n",
    "        for page in doc:\n",
    "            w, h = page.rect.width, page.rect.height\n",
    "            base_max = max(w, h)\n",
    "            zoom = max(min_zoom, min(max_zoom, max_dim / base_max))\n",
    "            mat = fitz.Matrix(zoom, zoom)\n",
    "            pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "            img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, pix.n)\n",
    "            yield img\n",
    "            del img\n",
    "            gc.collect()\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "def ocr_with_tiling(reader, img, tile_h: int, overlap: int, **ocr_kwargs) -> str:\n",
    "    \"\"\"OCR a tall image in vertical tiles to reduce peak memory.\"\"\"\n",
    "    H, _W = img.shape[:2]\n",
    "    y = 0\n",
    "    texts = []\n",
    "    while y < H:\n",
    "        y0 = max(0, y - overlap)\n",
    "        y1 = min(H, y + tile_h)\n",
    "        tile = img[y0:y1, :, :]\n",
    "        lines = reader.readtext(tile, detail=0, paragraph=True, **ocr_kwargs)\n",
    "        texts.append(\"\\n\".join(lines))\n",
    "        y += tile_h - overlap\n",
    "        del tile\n",
    "        gc.collect()\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "def ocr_pdf_to_md(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        import easyocr  # type: ignore\n",
    "    except ImportError:\n",
    "        return \"\"\n",
    "\n",
    "    reader = easyocr.Reader(LANGS, gpu=USE_GPU)\n",
    "    OCR_KW = dict(canvas_size=CANVAS_SIZE, mag_ratio=1.0, batch_size=1, workers=0)\n",
    "\n",
    "    page_md = []\n",
    "    for i, img in enumerate(iter_pdf_images(pdf_path, max_dim=MAX_DIM), start=1):\n",
    "        try:\n",
    "            lines = reader.readtext(img, detail=0, paragraph=True, **OCR_KW)\n",
    "            txt = \"\\n\".join(lines)\n",
    "        except RuntimeError as e:\n",
    "            if \"not enough memory\" in str(e):\n",
    "                txt = ocr_with_tiling(reader, img, tile_h=TILE_H, overlap=TILE_OVERLAP, **OCR_KW)\n",
    "            else:\n",
    "                raise\n",
    "        finally:\n",
    "            del img\n",
    "            gc.collect()\n",
    "\n",
    "        if txt.strip():\n",
    "            page_md.append(f\"## Page {i}\\n\\n{txt.strip()}\")\n",
    "\n",
    "    return (\"\\n\\n----\\n\\n\".join(page_md)).strip()\n",
    "\n",
    "\n",
    "# ----------------- Orchestration -----------------\n",
    "def pick_md(pdf_path: Path) -> str:\n",
    "    \"\"\"Try digital markdown; fallback to tables/text; then OCR.\"\"\"\n",
    "    # 1) Best: PyMuPDF4LLM\n",
    "    md = digital_md_pymupdf4llm(pdf_path)\n",
    "    if len(md) >= DIGITAL_MD_MINLEN:\n",
    "        return md\n",
    "\n",
    "    # 2) pdfplumber table+text\n",
    "    md2 = digital_md_pdfplumber(pdf_path)\n",
    "    if len(md2) >= DIGITAL_MD_MINLEN or (md2 and md == \"\"):\n",
    "        return md2\n",
    "\n",
    "    # 3) OCR fallback\n",
    "    md3 = ocr_pdf_to_md(pdf_path)\n",
    "    return md3 or md or md2  # return whatever we got\n",
    "\n",
    "\n",
    "def process_pdf(pdf_path: Path) -> Path:\n",
    "    out_md = pdf_path.with_suffix(\".md\")\n",
    "    if SKIP_EXISTING and out_md.exists():\n",
    "        print(f\"[SKIP] Already exists: {out_md.name}\")\n",
    "        return out_md\n",
    "\n",
    "    md_text = pick_md(pdf_path)\n",
    "    with open(out_md, \"w\", encoding=ENCODING, newline=\"\\n\") as f:\n",
    "        f.write(md_text or \"\")\n",
    "    print(f\"[DONE] {pdf_path.name} -> {out_md.name}\")\n",
    "    return out_md\n",
    "\n",
    "\n",
    "def main():\n",
    "    in_dir = Path(input_dir_str)\n",
    "    if not in_dir.exists():\n",
    "        raise FileNotFoundError(f\"Input folder not found: {in_dir.resolve()}\")\n",
    "\n",
    "    pdfs = sorted(in_dir.rglob(\"*.pdf\"))\n",
    "    if not pdfs:\n",
    "        print(f\"[INFO] No PDFs found under: {in_dir.resolve()}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Found {len(pdfs)} PDF(s) under {in_dir.resolve()}\\n\")\n",
    "    success, failed = 0, 0\n",
    "    for i, pdf_path in enumerate(pdfs, start=1):\n",
    "        try:\n",
    "            print(f\"[{i}/{len(pdfs)}] {pdf_path}\")\n",
    "            process_pdf(pdf_path)\n",
    "            success += 1\n",
    "        except Exception:\n",
    "            failed += 1\n",
    "            print(f\"[ERROR] Failed: {pdf_path.name}\")\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"\\n[SUMMARY] Success: {success}  Failed: {failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "453b07c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 10 PDF(s) under D:\\Users\\Dell\\Documents\\GeoSpark\\rag-app\\backend\\resource\n",
      "\n",
      "[1/10] ..\\..\\resource\\107b991c3b5e1bdc.pdf\n",
      "[SKIP] Already exists: 107b991c3b5e1bdc.md\n",
      "[2/10] ..\\..\\resource\\การปรับตัวรองรับการเปลี่ยนแปลงสภาพภูมิอากาศของเกษตรกรชาวสวนยางพาราจังหวัดระยอง.pdf\n",
      "[SKIP] Already exists: การปรับตัวรองรับการเปลี่ยนแปลงสภาพภูมิอากาศของเกษตรกรชาวสวนยางพาราจังหวัดระยอง.md\n",
      "[3/10] ..\\..\\resource\\ปัจจัยที่มีอิทธิพลต่อการปรับตัวของเกษตรกรชาวสวนยางพาราต่อการเปลี่ยนแปลงสภาพภูมิอากาสในพื้นที่ภาคใต้ตอนล่างของประเทศไทย.pdf\n",
      "[SKIP] Already exists: ปัจจัยที่มีอิทธิพลต่อการปรับตัวของเกษตรกรชาวสวนยางพาราต่อการเปลี่ยนแปลงสภาพภูมิอากาสในพื้นที่ภาคใต้ตอนล่างของประเทศไทย.md\n",
      "[4/10] ..\\..\\resource\\ผลกระทบของการเปลี่ยนแปลงสภาพภูมิอากาศต่อผลผลิตยางพาราในพื้นที่เขตภาคใต้ตอนล่างของประเทศไทย.pdf\n",
      "[SKIP] Already exists: ผลกระทบของการเปลี่ยนแปลงสภาพภูมิอากาศต่อผลผลิตยางพาราในพื้นที่เขตภาคใต้ตอนล่างของประเทศไทย.md\n",
      "[5/10] ..\\..\\resource\\มาตรการชะลอการเก็บเกี่ยวมันสำปะหลังปี2566_67.pdf\n",
      "[SKIP] Already exists: มาตรการชะลอการเก็บเกี่ยวมันสำปะหลังปี2566_67.md\n",
      "[6/10] ..\\..\\resource\\มาตรการรักษาเสถียรภาพราคามันสำปะหลังปี2567_2568.pdf\n",
      "[SKIP] Already exists: มาตรการรักษาเสถียรภาพราคามันสำปะหลังปี2567_2568.md\n",
      "[7/10] ..\\..\\resource\\รายงานการเปลี่ยนแปลงสภาพภูมิอากาศ.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n",
      "d:\\Users\\Dell\\Documents\\GeoSpark\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] รายงานการเปลี่ยนแปลงสภาพภูมิอากาศ.pdf -> รายงานการเปลี่ยนแปลงสภาพภูมิอากาศ.md\n",
      "[8/10] ..\\..\\resource\\สถานการณ์ยางพฤษภาคม2568.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] สถานการณ์ยางพฤษภาคม2568.pdf -> สถานการณ์ยางพฤษภาคม2568.md\n",
      "[9/10] ..\\..\\resource\\แผนปฏิบัติการประจําปีงบประมาณปี2567ของการยางแห่งประเทศไทย.pdf\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
