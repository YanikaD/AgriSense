{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neo4j Document Processing Components\n",
    "\n",
    "Core components for processing documents with embeddings and ingesting into Neo4j.\n",
    "Use these components to loop through files in a folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (5.28.2)\n",
      "Requirement already satisfied: sentence-transformers in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: pandas in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: scikit-learn in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: pytz in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from neo4j) (2025.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from sentence-transformers) (4.55.4)\n",
      "Requirement already satisfied: tqdm in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scipy in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\users\\dell\\documents\\geospark\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install neo4j sentence-transformers pandas numpy scikit-learn python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Dell\\Documents\\GeoSpark\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment variables loaded from .env file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# LLM imports\n",
    "try:\n",
    "    import openai\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "    print(\"‚ö† OpenAI not installed. Install with: pip install openai\")\n",
    "\n",
    "# Load environment variables\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"‚úì Environment variables loaded from .env file\")\n",
    "except ImportError:\n",
    "    print(\"‚ö† python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    print(\"Using environment variables directly from system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Create a `.env` file in your project directory with:\n",
    "```\n",
    "NEO4J_URI=bolt://localhost:7687\n",
    "NEO4J_USERNAME=neo4j\n",
    "NEO4J_PASSWORD=your_actual_password\n",
    "EMBEDDING_MODEL=paraphrase-multilingual-MiniLM-L12-v2\n",
    "CHUNK_DELIMITER=---\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment variables loaded from: D:\\Users\\Dell\\Documents\\GeoSpark\\rag-app\\backend\\.env\n",
      "‚úÖ Configuration loaded:\n",
      "  Neo4j URI: neo4j+s://f9af1b11.databases.neo4j.io\n",
      "  Neo4j Username: neo4j\n",
      "  Chunk Delimiter: '---'\n",
      "  Embedding Model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "  LLM Provider: openai\n",
      "  LLM Model: gpt-5-nano\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from specific path\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    # Specify the path to your .env file\n",
    "    env_path = r'D:\\Users\\Dell\\Documents\\GeoSpark\\rag-app\\backend\\.env'\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"‚úì Environment variables loaded from: {env_path}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö† python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    print(\"Using environment variables directly from system\")\n",
    "\n",
    "# Neo4j Configuration from environment variables\n",
    "NEO4J_URI = os.getenv('NEO4J_URI', 'bolt://localhost:7687')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME', 'neo4j')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')   \n",
    "# Document configuration\n",
    "CHUNK_DELIMITER = os.getenv('CHUNK_DELIMITER', '---')\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'paraphrase-multilingual-MiniLM-L12-v2')\n",
    "# LLM configuration\n",
    "LLM_PROVIDER = os.getenv('LLM_PROVIDER', 'openai')  # 'openai' or 'anthropic'\n",
    "LLM_MODEL = os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "# Validate required environment variables\n",
    "if not NEO4J_PASSWORD:\n",
    "    raise ValueError(\"NEO4J_PASSWORD not found in environment variables. Please check your .env file.\")\n",
    "print(f\"‚úÖ Configuration loaded:\")\n",
    "print(f\"  Neo4j URI: {NEO4J_URI}\")\n",
    "print(f\"  Neo4j Username: {NEO4J_USERNAME}\")\n",
    "print(f\"  Chunk Delimiter: '{CHUNK_DELIMITER}'\")\n",
    "print(f\"  Embedding Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"  LLM Provider: {LLM_PROVIDER}\")\n",
    "print(f\"  LLM Model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_patterns = {\n",
    "        'title': ['factors influencing', 'executive summary', '‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°', '‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå'],\n",
    "        'abstract': ['‡∏ö‡∏ó‡∏Ñ‡∏±‡∏î‡∏¢‡πà‡∏≠', 'abstract', '‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ'],\n",
    "        'introduction': ['‡∏ö‡∏ó‡∏ô‡∏≥', 'introduction'],\n",
    "        'background': ['‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô', 'background', '‡∏ó‡∏µ‡πà‡∏°‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç'],\n",
    "        'methodology': ['‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏à‡∏±‡∏¢', 'methodology', '‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£'],\n",
    "        'results': ['‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏à‡∏±‡∏¢', 'results', '‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'],\n",
    "        'discussion': ['‡∏≠‡∏†‡∏¥‡∏õ‡∏£‡∏≤‡∏¢‡∏ú‡∏•', 'discussion'],\n",
    "        'conclusion': ['‡∏™‡∏£‡∏∏‡∏õ', 'conclusion', '‡∏ö‡∏ó‡∏™‡∏£‡∏∏‡∏õ'],\n",
    "        'recommendations': ['‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞', 'recommendations', '‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠'],\n",
    "        'measures': ['‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£', 'measures', '‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç', '‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£'],\n",
    "        'policy': ['‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢', 'policy', '‡∏°‡∏ï‡∏¥', '‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ'],\n",
    "        'forecast': ['‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå', 'forecast', '‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°', '‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå'],\n",
    "        'situation': ['‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå', 'situation', '‡∏†‡∏≤‡∏ß‡∏∞'],\n",
    "        'market': ['‡∏ï‡∏•‡∏≤‡∏î', 'market', '‡∏£‡∏≤‡∏Ñ‡∏≤', '‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å'],\n",
    "        'climate': ['‡∏™‡∏†‡∏≤‡∏û‡∏†‡∏π‡∏°‡∏¥‡∏≠‡∏≤‡∏Å‡∏≤‡∏®', 'climate', '‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥', '‡∏ù‡∏ô', '‡πÄ‡∏≠‡∏•‡∏ô‡∏µ‡πÇ‡∏ç', '‡∏•‡∏≤‡∏ô‡∏µ‡∏ç‡∏≤'],\n",
    "        'agriculture': ['‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£', 'agriculture', '‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏≤‡∏∞‡∏õ‡∏•‡∏π‡∏Å', '‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï'],\n",
    "        'forecast': ['‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå', 'forecast', '‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°', '‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå'],\n",
    "        'appendix': ['‡∏†‡∏≤‡∏Ñ‡∏ú‡∏ô‡∏ß‡∏Å', 'appendix']\n",
    "    }\n",
    "    \n",
    "    # Crop type patterns - these will be added as both section_type and crop_type\n",
    "crop_patterns = {\n",
    "        'rubber': ['‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤', 'rubber', '‡∏¢‡∏≤‡∏á', '‡∏™‡∏ß‡∏ô‡∏¢‡∏≤‡∏á','‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ö', '‡∏¢‡∏≤‡∏á‡πÅ‡∏ú‡πà‡∏ô‡∏î‡∏¥‡∏ö'],\n",
    "        'cassava': ['‡∏°‡∏±‡∏ô‡∏™‡∏≥‡∏õ‡∏∞‡∏´‡∏•‡∏±‡∏á', 'cassava', '‡∏´‡∏±‡∏ß‡∏°‡∏±‡∏ô', '‡πÅ‡∏õ‡πâ‡∏á‡∏°‡∏±‡∏ô', '‡∏°‡∏±‡∏ô‡πÄ‡∏™‡πâ‡∏ô']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Section Extractor initialized with openai (gpt-5-nano)\n"
     ]
    }
   ],
   "source": [
    "class LLMSectionExtractor:\n",
    "        def __init__(self, provider: str = 'openai', model: str = 'gpt-3.5-turbo'):\n",
    "            self.provider = provider.lower()\n",
    "            self.model = model\n",
    "            \n",
    "            if self.provider == 'openai':\n",
    "                if not OPENAI_API_KEY:\n",
    "                    raise ValueError(\"OpenAI API key not found or package not installed\")\n",
    "                openai.api_key = OPENAI_API_KEY\n",
    "                self.client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported LLM provider: {provider}\")\n",
    "            \n",
    "            print(f\"‚úÖ LLM Section Extractor initialized with {provider} ({model})\")\n",
    "        \n",
    "        def extract_section_info(self, content: str, max_retries: int = 3) -> Dict[str, Any]:\n",
    "            \"\"\"\n",
    "            Extract section information from content using LLM\n",
    "            \n",
    "            Returns:\n",
    "                Dict containing section_type, section_title, key_topics, and summary\n",
    "            \"\"\"\n",
    "            prompt = f\"\"\"\n",
    "    Analyze the following text chunk and extract structural information. Return a JSON object with these fields:\n",
    "    - \"section_type\": The list of type of section, which in {section_patterns}, if none match, return []\n",
    "    - \"crop_type\": The crop type, which in {crop_patterns}, if there are no crop type mentioned, return []\n",
    "    - \"key_topics\": An array of 3-5 main topics or keywords mentioned in this chunk\n",
    "    - \"organization\": Any organizations mentioned, if none, return []\n",
    "\n",
    "    \n",
    "    Text chunk:\n",
    "    {content}\n",
    "    \n",
    "    Return only valid JSON:\n",
    "    \"\"\"\n",
    "            \n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    if self.provider == 'openai':\n",
    "                        response = self.client.chat.completions.create(\n",
    "                            model=self.model,\n",
    "                            messages=[\n",
    "                                {\"role\": \"system\", \"content\": \"You are an expert at analyzing document structure and extracting section information. Always return valid JSON.\"},\n",
    "                                {\"role\": \"user\", \"content\": prompt}\n",
    "                            ]\n",
    "                        )\n",
    "                        response_text = response.choices[0].message.content.strip()\n",
    "                    \n",
    "                    \n",
    "                    # Clean and parse JSON response\n",
    "                    if response_text.startswith('```json'):\n",
    "                        response_text = response_text.split('```json')[1].split('```')[0].strip()\n",
    "                    elif response_text.startswith('```'):\n",
    "                        response_text = response_text.split('```')[1].strip()\n",
    "                    \n",
    "                    result = json.loads(response_text)\n",
    "                    \n",
    "                    # Validate required fields\n",
    "                    required_fields = ['section_type', 'crop_type', 'key_topics', 'organization']\n",
    "                    if all(field in result for field in required_fields):\n",
    "                        return result\n",
    "                    else:\n",
    "                        print(f\"‚ö† Missing required fields in LLM response, attempt {attempt + 1}\")\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ö† JSON parsing error on attempt {attempt + 1}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö† LLM API error on attempt {attempt + 1}: {e}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            \n",
    "            # Fallback if all attempts fail\n",
    "            print(\"‚ö† All LLM attempts failed, using fallback section info\")\n",
    "            return {\n",
    "                \"section_type\": [],\n",
    "                \"crop_type\": [],\n",
    "                \"key_topics\": [],\n",
    "                \"organization\": []\n",
    "            }\n",
    "        \n",
    "        def extract_batch_section_info(self, chunks: List[Dict[str, Any]], batch_size: int = 5) -> List[Dict[str, Any]]:\n",
    "            \"\"\"\n",
    "            Extract section info for multiple chunks with rate limiting\n",
    "            \"\"\"\n",
    "            print(f\"Extracting section information for {len(chunks)} chunks...\")\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                try:\n",
    "                    section_info = self.extract_section_info(chunk['content'])\n",
    "                    \n",
    "                    # Add section info to chunk\n",
    "                    chunk.update(section_info)\n",
    "                    \n",
    "                    if (i + 1) % batch_size == 0:\n",
    "                        print(f\"  Processed {i + 1}/{len(chunks)} chunks\")\n",
    "                        time.sleep(1)  # Rate limiting\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö† Error processing chunk {i + 1}: {e}\")\n",
    "                    # Add fallback section info\n",
    "                    chunk.update({\n",
    "                        \"section_type\": [],\n",
    "                        \"crop_type\": [],\n",
    "                        \"key_topics\": [],\n",
    "                        \"organization\": []\n",
    "                    })\n",
    "            \n",
    "            print(\"‚úÖ Section extraction completed\")\n",
    "            return chunks\n",
    "    \n",
    "    # Initialize LLM extractor if keys are available\n",
    "llm_extractor = None\n",
    "try:\n",
    "    llm_extractor = LLMSectionExtractor(LLM_PROVIDER, LLM_MODEL)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Could not initialize LLM extractor: {e}\")\n",
    "    print(\"Section extraction will be skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model: paraphrase-multilingual-MiniLM-L12-v2\n",
      "‚úì Model loaded successfully. Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name: str = EMBEDDING_MODEL):\n",
    "        print(f\"Loading SentenceTransformer model: {model_name}\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "        print(f\"‚úì Model loaded successfully. Embedding dimension: {self.embedding_dim}\")\n",
    "    \n",
    "    def generate_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for a single text\"\"\"\n",
    "        embedding = self.model.encode(text, normalize_embeddings=True)\n",
    "        return embedding.tolist()\n",
    "    \n",
    "    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for a batch of texts\"\"\"\n",
    "        embeddings = self.model.encode(texts, normalize_embeddings=True, show_progress_bar=True)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    def compute_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:\n",
    "        \"\"\"Compute cosine similarity between two embeddings\"\"\"\n",
    "        embedding1 = np.array(embedding1).reshape(1, -1)\n",
    "        embedding2 = np.array(embedding2).reshape(1, -1)\n",
    "        return cosine_similarity(embedding1, embedding2)[0][0]\n",
    "\n",
    "# Initialize embedding generator\n",
    "embedding_generator = EmbeddingGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced document processor initialized\n"
     ]
    }
   ],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, delimiter: str = '---', embedding_generator=None, llm_extractor=None):\n",
    "        self.delimiter = delimiter\n",
    "        self.embedding_generator = embedding_generator\n",
    "        self.llm_extractor = llm_extractor\n",
    "    \n",
    "    def read_document(self, file_path: str) -> str:\n",
    "        \"\"\"Read document from file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                return file.read()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def chunk_document(self, text: str, generate_embeddings: bool = True, extract_sections: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Split document into chunks using delimiter and optionally generate embeddings\"\"\"\n",
    "        # Split by delimiter\n",
    "        chunks = text.split(self.delimiter)\n",
    "        \n",
    "        processed_chunks = []\n",
    "        chunk_texts = []\n",
    "        \n",
    "        # First pass: create chunk metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            cleaned_chunk = chunk.strip()\n",
    "            \n",
    "            if cleaned_chunk:  # Only process non-empty chunks\n",
    "                chunk_data = {\n",
    "                    'id': str(uuid.uuid4()),\n",
    "                    'sequence': i + 1,\n",
    "                    'content': cleaned_chunk,\n",
    "                    'word_count': len(cleaned_chunk.split()),\n",
    "                    'char_count': len(cleaned_chunk),\n",
    "                    'created_at': datetime.now().isoformat()\n",
    "                }\n",
    "                processed_chunks.append(chunk_data)\n",
    "                chunk_texts.append(cleaned_chunk)\n",
    "        \n",
    "        # Second pass: generate embeddings in batch if requested\n",
    "        if generate_embeddings and self.embedding_generator and chunk_texts:\n",
    "            print(f\"Generating embeddings for {len(chunk_texts)} chunks...\")\n",
    "            embeddings = self.embedding_generator.generate_embeddings_batch(chunk_texts)\n",
    "            \n",
    "            # Add embeddings to chunk data\n",
    "            for i, embedding in enumerate(embeddings):\n",
    "                processed_chunks[i]['embedding'] = embedding\n",
    "                processed_chunks[i]['embedding_dim'] = len(embedding)\n",
    "            \n",
    "            print(f\"‚úì Embeddings generated successfully\")\n",
    "        \n",
    "        # Third pass: extract section information using LLM\n",
    "        if extract_sections and self.llm_extractor and processed_chunks:\n",
    "            processed_chunks = self.llm_extractor.extract_batch_section_info(processed_chunks)\n",
    "        elif extract_sections and not self.llm_extractor:\n",
    "            print(\"‚ö† LLM extractor not available, skipping section extraction\")\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "\n",
    "# Initialize processor with LLM extractor\n",
    "processor = DocumentProcessor(CHUNK_DELIMITER, embedding_generator, llm_extractor)\n",
    "print(\"‚úÖ Enhanced document processor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neo4j Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_list(value):\n",
    "    \"\"\"Force value into a list ([], [value], or value if already list).\"\"\"\n",
    "    if value is None or value == \"\":\n",
    "        return []\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    return [value]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Connected to Neo4j successfully\n"
     ]
    }
   ],
   "source": [
    "class Neo4jHandler:\n",
    "    def __init__(self, uri: str, username: str, password: str):\n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "            # Test connection\n",
    "            with self.driver.session() as session:\n",
    "                session.run(\"RETURN 1\")\n",
    "            print(\"‚úì Connected to Neo4j successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to connect to Neo4j: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def close(self):\n",
    "        if hasattr(self, 'driver'):\n",
    "            self.driver.close()\n",
    "    \n",
    "    def create_schema(self):\n",
    "        \"\"\"Create indexes and constraints for better performance\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            queries = [\n",
    "                \"CREATE CONSTRAINT document_id IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE\",\n",
    "                \"CREATE CONSTRAINT chunk_id IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\",\n",
    "                \"CREATE INDEX chunk_sequence IF NOT EXISTS FOR (c:Chunk) ON (c.sequence)\",\n",
    "                \"CREATE INDEX document_title IF NOT EXISTS FOR (d:Document) ON (d.title)\",\n",
    "                \"CREATE INDEX document_filename IF NOT EXISTS FOR (d:Document) ON (d.filename)\",\n",
    "                \"CREATE INDEX chunk_section_type IF NOT EXISTS FOR (c:Chunk) ON (c.section_type)\",\n",
    "                \"CREATE INDEX chunk_crop_type IF NOT EXISTS FOR (c:Chunk) ON (c.crop_type)\",\n",
    "                \"CREATE INDEX chunk_organization IF NOT EXISTS FOR (c:Chunk) ON (c.organization)\",\n",
    "                \"CREATE INDEX chunk_key_topics IF NOT EXISTS FOR (c:Chunk) ON (c.key_topics)\",\n",
    "                # Text search index for content\n",
    "                \"CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS FOR (c:Chunk) ON EACH [c.content]\"\n",
    "            ]\n",
    "            \n",
    "            for query in queries:\n",
    "                try:\n",
    "                    session.run(query)\n",
    "                    print(f\"‚úì Executed: {query.split()[1]} {query.split()[2]}\")\n",
    "                except Exception as e:\n",
    "                    if \"already exists\" not in str(e).lower():\n",
    "                        print(f\"‚ö† Warning: {e}\")\n",
    "    \n",
    "    def create_document_node(self, document_data: Dict[str, Any]):\n",
    "        \"\"\"Create a document node\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            CREATE (d:Document {\n",
    "                id: $id,\n",
    "                title: $title,\n",
    "                filename: $filename,\n",
    "                file_path: $file_path,\n",
    "                total_chunks: $total_chunks,\n",
    "                created_at: $created_at\n",
    "            })\n",
    "            RETURN d\n",
    "            \"\"\"\n",
    "            result = session.run(query, **document_data)\n",
    "            return result.single()\n",
    "    \n",
    "    def create_chunk_nodes(self, chunks: List[Dict[str, Any]], document_id: str, batch_size: int = 5, sleep_time: int = 10):\n",
    "        \"\"\"\n",
    "        Create chunk nodes with embeddings and link them to document, in batches.\n",
    "        \"\"\"\n",
    "        total = len(chunks)\n",
    "        for i in range(0, total, batch_size):\n",
    "            batch = chunks[i:i + batch_size]\n",
    "\n",
    "            with self.driver.session() as session:\n",
    "                for j, chunk in enumerate(batch, start=i+1):\n",
    "                    # Prepare chunk data\n",
    "                    chunk_data = {\n",
    "                        'id': chunk['id'],\n",
    "                        'sequence': chunk['sequence'],\n",
    "                        'content': chunk['content'],\n",
    "                        'word_count': chunk['word_count'],\n",
    "                        'char_count': chunk['char_count'],\n",
    "                        'created_at': chunk['created_at'],\n",
    "                        'embedding': chunk['embedding'],\n",
    "                        'embedding_dim': chunk['embedding_dim'],\n",
    "                        'section_type': ensure_list(chunk.get('section_type', [])),\n",
    "                        'crop_type': ensure_list(chunk.get('crop_type', [])),\n",
    "                        'key_topics': ensure_list(chunk.get('key_topics', [])),\n",
    "                        'organization': ensure_list(chunk.get('organization', []))\n",
    "                    }\n",
    "\n",
    "                    # Create chunk node\n",
    "                    chunk_query = \"\"\"\n",
    "                    CREATE (c:Chunk {\n",
    "                        id: $id,\n",
    "                        sequence: $sequence,\n",
    "                        content: $content,\n",
    "                        word_count: $word_count,\n",
    "                        char_count: $char_count,\n",
    "                        created_at: $created_at,\n",
    "                        embedding: $embedding,\n",
    "                        embedding_dim: $embedding_dim,\n",
    "                        section_type: $section_type,\n",
    "                        crop_type: $crop_type,\n",
    "                        key_topics: $key_topics,\n",
    "                        organization: $organization\n",
    "                    })\n",
    "                    \"\"\"\n",
    "                    session.run(chunk_query, chunk_data)\n",
    "\n",
    "                    # Link to document\n",
    "                    link_query = \"\"\"\n",
    "                    MATCH (d:Document {id: $doc_id})\n",
    "                    MATCH (c:Chunk {id: $chunk_id})\n",
    "                    CREATE (d)-[:HAS_CHUNK {sequence: $sequence}]->(c)\n",
    "                    \"\"\"\n",
    "                    session.run(link_query, {\n",
    "                        'doc_id': document_id,\n",
    "                        'chunk_id': chunk['id'],\n",
    "                        'sequence': chunk['sequence']\n",
    "                    })\n",
    "\n",
    "                    # NEXT relationship\n",
    "                    if chunk['sequence'] > 1:\n",
    "                        next_query = \"\"\"\n",
    "                        MATCH (d:Document {id: $doc_id})-[:HAS_CHUNK]->(c1:Chunk {sequence: $prev_seq})\n",
    "                        MATCH (d)-[:HAS_CHUNK]->(c2:Chunk {sequence: $curr_seq})\n",
    "                        CREATE (c1)-[:NEXT_CHUNK]->(c2)\n",
    "                        \"\"\"\n",
    "                        session.run(next_query, {\n",
    "                            'doc_id': document_id,\n",
    "                            'prev_seq': chunk['sequence'] - 1,\n",
    "                            'curr_seq': chunk['sequence']\n",
    "                        })\n",
    "\n",
    "                print(f\"  ‚úÖ Batch {i//batch_size + 1}: processed {i + len(batch)}/{total} chunks\")\n",
    "\n",
    "            # sleep between batches (except after last one)\n",
    "            if i + batch_size < total:\n",
    "                time.sleep(sleep_time)\n",
    "    \n",
    "    def semantic_search(self, query_embedding: List[float], limit: int = 5, similarity_threshold: float = 0.7):\n",
    "        \"\"\"Perform semantic search using embeddings\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            MATCH (c:Chunk)\n",
    "            WHERE c.embedding IS NOT NULL\n",
    "            RETURN c.id, c.content, c.sequence, c.embedding\n",
    "            \"\"\"\n",
    "            \n",
    "            result = session.run(query)\n",
    "            chunks_with_similarity = []\n",
    "            \n",
    "            for record in result:\n",
    "                chunk_embedding = record['c.embedding']\n",
    "                similarity = cosine_similarity(\n",
    "                    np.array(query_embedding).reshape(1, -1),\n",
    "                    np.array(chunk_embedding).reshape(1, -1)\n",
    "                )[0][0]\n",
    "                \n",
    "                if similarity >= similarity_threshold:\n",
    "                    chunks_with_similarity.append({\n",
    "                        'id': record['c.id'],\n",
    "                        'content': record['c.content'],\n",
    "                        'sequence': record['c.sequence'],\n",
    "                        'similarity': float(similarity)\n",
    "                    })\n",
    "            \n",
    "            # Sort by similarity and return top results\n",
    "            chunks_with_similarity.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "            return chunks_with_similarity[:limit]\n",
    "    \n",
    "    def get_document_statistics(self):\n",
    "        \"\"\"Get statistics about the ingested data\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            stats_query = \"\"\"\n",
    "            MATCH (d:Document)\n",
    "            OPTIONAL MATCH (d)-[:HAS_CHUNK]->(c:Chunk)\n",
    "            RETURN \n",
    "                count(DISTINCT d) as total_documents,\n",
    "                count(c) as total_chunks,\n",
    "                count(CASE WHEN c.embedding IS NOT NULL THEN 1 END) as chunks_with_embeddings\n",
    "            \"\"\"\n",
    "            \n",
    "            result = session.run(stats_query)\n",
    "            return result.single()\n",
    "\n",
    "# Initialize Neo4j handler\n",
    "neo4j_handler = Neo4jHandler(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Neo4j schema...\n",
      "‚úì Executed: CONSTRAINT document_id\n",
      "‚úì Executed: CONSTRAINT chunk_id\n",
      "‚úì Executed: INDEX chunk_sequence\n",
      "‚úì Executed: INDEX document_title\n",
      "‚úì Executed: INDEX document_filename\n",
      "‚úì Executed: INDEX chunk_section_type\n",
      "‚úì Executed: INDEX chunk_crop_type\n",
      "‚úì Executed: INDEX chunk_organization\n",
      "‚úì Executed: INDEX chunk_key_topics\n",
      "‚úì Executed: FULLTEXT INDEX\n",
      "‚úì Schema setup completed\n"
     ]
    }
   ],
   "source": [
    "# Create schema once\n",
    "print(\"Setting up Neo4j schema...\")\n",
    "neo4j_handler.create_schema()\n",
    "print(\"‚úì Schema setup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. File Processing Function\n",
    "\n",
    "Use this function to process individual files in your loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì File processing function ready\n"
     ]
    }
   ],
   "source": [
    "def process_single_file(file_path: str, title: str = None) -> bool:\n",
    "    \"\"\"\n",
    "    Process a single file and ingest into Neo4j\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the document file\n",
    "        title (str): Optional title for the document\n",
    "        \n",
    "    Returns:\n",
    "        bool: Success status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nüìÑ Processing: {file_path}\")\n",
    "        \n",
    "        # Read document\n",
    "        document_text = processor.read_document(file_path)\n",
    "        if not document_text.strip():\n",
    "            print(f\"‚ö† Warning: Empty document - {file_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Process chunks with embeddings\n",
    "        chunks = processor.chunk_document(document_text, generate_embeddings=True)\n",
    "        if not chunks:\n",
    "            print(f\"‚ö† Warning: No chunks generated - {file_path}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"‚úì Generated {len(chunks)} chunks\")\n",
    "        \n",
    "        # Create document metadata\n",
    "        document_id = str(uuid.uuid4())\n",
    "        filename = os.path.basename(file_path)\n",
    "        \n",
    "        document_data = {\n",
    "            'id': document_id,\n",
    "            'title': title or filename,\n",
    "            'filename': filename,\n",
    "            'file_path': file_path,\n",
    "            'total_chunks': len(chunks),\n",
    "            'created_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Create document node\n",
    "        neo4j_handler.create_document_node(document_data)\n",
    "        print(f\"‚úì Document node created: {document_id}\")\n",
    "        \n",
    "        # Create chunk nodes and relationships\n",
    "        neo4j_handler.create_chunk_nodes(chunks, document_id)\n",
    "        print(f\"‚úì {len(chunks)} chunks ingested successfully\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úì File processing function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Batch Processing Function\n",
    "\n",
    "Use this to process multiple files from a folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Batch processing function ready\n"
     ]
    }
   ],
   "source": [
    "def process_folder(folder_path: str, file_extensions: List[str] = ['.txt', '.md'], max_files: int = None):\n",
    "    \"\"\"\n",
    "    Process all files in a folder\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing documents\n",
    "        file_extensions (List[str]): List of file extensions to process\n",
    "        max_files (int): Optional limit on number of files to process\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"‚ùå Folder not found: {folder_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get all matching files\n",
    "    files = []\n",
    "    for ext in file_extensions:\n",
    "        pattern = os.path.join(folder_path, f\"*{ext}\")\n",
    "        import glob\n",
    "        files.extend(glob.glob(pattern))\n",
    "    \n",
    "    if max_files:\n",
    "        files = files[:max_files]\n",
    "    \n",
    "    print(f\"üìÅ Found {len(files)} files to process\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, file_path in enumerate(files, 1):\n",
    "        print(f\"\\n[{i}/{len(files)}] Processing file...\")\n",
    "        \n",
    "        if process_single_file(file_path):\n",
    "            successful += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\nüìä Batch Processing Summary:\")\n",
    "    print(f\"  Total files: {len(files)}\")\n",
    "    print(f\"  Successful: {successful}\")\n",
    "    print(f\"  Failed: {failed}\")\n",
    "    \n",
    "    # Show final statistics\n",
    "    stats = neo4j_handler.get_document_statistics()\n",
    "    print(f\"\\nüìà Database Statistics:\")\n",
    "    print(f\"  Total Documents: {stats['total_documents']}\")\n",
    "    print(f\"  Total Chunks: {stats['total_chunks']}\")\n",
    "    print(f\"  Chunks with Embeddings: {stats['chunks_with_embeddings']}\")\n",
    "\n",
    "print(\"‚úì Batch processing function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Example Usage\n",
    "\n",
    "Here's how to use the components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 12 files to process\n",
      "\n",
      "[1/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\4‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏°‡∏±‡∏ô.txt\n",
      "Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 1 chunks\n",
      "‚úì Document node created: 42bd1a09-7b29-4377-80c8-f6b7ba24109b\n",
      "  ‚úÖ Batch 1: processed 1/1 chunks\n",
      "‚úì 1 chunks ingested successfully\n",
      "\n",
      "[2/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡∏Å‡∏£‡∏∞‡∏ó‡∏£‡∏ß‡∏á‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡πÄ‡∏£‡πà‡∏á‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏•‡∏±‡∏Å‡∏•‡∏≠‡∏ö‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡πÄ‡∏ñ‡∏∑‡πà‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏ï‡∏Å‡∏ï‡πà‡∏≥.txt\n",
      "Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 1 chunks\n",
      "‚úì Document node created: 9e26b62f-faf5-451c-ab51-ea3d4fb59256\n",
      "  ‚úÖ Batch 1: processed 1/1 chunks\n",
      "‚úì 1 chunks ingested successfully\n",
      "\n",
      "[3/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏†‡∏π‡∏°‡∏¥‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡∏ä‡∏≤‡∏ß‡∏™‡∏ß‡∏ô‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏¢‡∏≠‡∏á.txt\n",
      "Generating embeddings for 11 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 11 chunks...\n",
      "  Processed 5/11 chunks\n",
      "  Processed 10/11 chunks\n",
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 11 chunks\n",
      "‚úì Document node created: a8cf4c3e-4a1e-4d0b-af2d-0e95c0bcea39\n",
      "  ‚úÖ Batch 1: processed 5/11 chunks\n",
      "  ‚úÖ Batch 2: processed 10/11 chunks\n",
      "  ‚úÖ Batch 3: processed 11/11 chunks\n",
      "‚úì 11 chunks ingested successfully\n",
      "\n",
      "[4/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¥‡∏ó‡∏ò‡∏¥‡∏û‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏Å‡∏£‡∏ä‡∏≤‡∏ß‡∏™‡∏ß‡∏ô‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏†‡∏π‡∏°‡∏¥‡∏≠‡∏≤‡∏Å‡∏≤‡∏™‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ï‡πâ‡∏ï‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢.txt\n",
      "Generating embeddings for 44 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 44 chunks...\n",
      "  Processed 5/44 chunks\n",
      "  Processed 10/44 chunks\n",
      "  Processed 15/44 chunks\n",
      "  Processed 20/44 chunks\n",
      "  Processed 25/44 chunks\n",
      "  Processed 30/44 chunks\n",
      "  Processed 35/44 chunks\n",
      "  Processed 40/44 chunks\n",
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 44 chunks\n",
      "‚úì Document node created: ea750f27-b236-45f1-86fb-b1820ffd5cf5\n",
      "  ‚úÖ Batch 1: processed 5/44 chunks\n",
      "  ‚úÖ Batch 2: processed 10/44 chunks\n",
      "  ‚úÖ Batch 3: processed 15/44 chunks\n",
      "  ‚úÖ Batch 4: processed 20/44 chunks\n",
      "  ‚úÖ Batch 5: processed 25/44 chunks\n",
      "  ‚úÖ Batch 6: processed 30/44 chunks\n",
      "  ‚úÖ Batch 7: processed 35/44 chunks\n",
      "  ‚úÖ Batch 8: processed 40/44 chunks\n",
      "  ‚úÖ Batch 9: processed 44/44 chunks\n",
      "‚úì 44 chunks ingested successfully\n",
      "\n",
      "[5/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏†‡∏π‡∏°‡∏¥‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏ï‡πà‡∏≠‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏ï‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ï‡πâ‡∏ï‡∏≠‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢.txt\n",
      "Generating embeddings for 24 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 24 chunks...\n",
      "  Processed 5/24 chunks\n",
      "  Processed 10/24 chunks\n",
      "  Processed 15/24 chunks\n",
      "  Processed 20/24 chunks\n",
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 24 chunks\n",
      "‚úì Document node created: a9ef4c13-667b-4662-8a1e-25c85c2a84f2\n",
      "  ‚úÖ Batch 1: processed 5/24 chunks\n",
      "  ‚úÖ Batch 2: processed 10/24 chunks\n",
      "  ‚úÖ Batch 3: processed 15/24 chunks\n",
      "  ‚úÖ Batch 4: processed 20/24 chunks\n",
      "  ‚úÖ Batch 5: processed 24/24 chunks\n",
      "‚úì 24 chunks ingested successfully\n",
      "\n",
      "[6/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡∏ú‡∏•‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ú‡∏•‡∏ú‡∏•‡∏¥‡∏ï‡∏°‡∏±‡∏ô‡∏™‡∏≥‡∏õ‡∏∞‡∏´‡∏•‡∏±‡∏á‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô.txt\n",
      "Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 1 chunks\n",
      "‚úì Document node created: 212790eb-b95a-468e-8f5a-efe72f944415\n",
      "  ‚úÖ Batch 1: processed 1/1 chunks\n",
      "‚úì 1 chunks ingested successfully\n",
      "\n",
      "[7/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏ä‡∏∞‡∏•‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏°‡∏±‡∏ô‡∏™‡∏≥‡∏õ‡∏∞‡∏´‡∏•‡∏±‡∏á‡∏õ‡∏µ2566_67.txt\n",
      "Generating embeddings for 2 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 2 chunks...\n",
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 2 chunks\n",
      "‚úì Document node created: 118e0286-7521-4181-bfef-b8756063a98d\n",
      "  ‚úÖ Batch 1: processed 2/2 chunks\n",
      "‚úì 2 chunks ingested successfully\n",
      "\n",
      "[8/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏†‡∏π‡∏°‡∏¥‡∏≠‡∏≤‡∏Å‡∏≤‡∏®.txt\n",
      "Generating embeddings for 56 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 56 chunks...\n",
      "  Processed 5/56 chunks\n",
      "  Processed 10/56 chunks\n",
      "  Processed 15/56 chunks\n",
      "  Processed 20/56 chunks\n",
      "  Processed 25/56 chunks\n",
      "  Processed 30/56 chunks\n",
      "  Processed 35/56 chunks\n",
      "  Processed 40/56 chunks\n",
      "  Processed 45/56 chunks\n",
      "  Processed 50/56 chunks\n",
      "  Processed 55/56 chunks\n",
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 56 chunks\n",
      "‚úì Document node created: c0a6239e-0e09-4791-af98-be0822b6a69c\n",
      "  ‚úÖ Batch 1: processed 5/56 chunks\n",
      "  ‚úÖ Batch 2: processed 10/56 chunks\n",
      "  ‚úÖ Batch 3: processed 15/56 chunks\n",
      "  ‚úÖ Batch 4: processed 20/56 chunks\n",
      "  ‚úÖ Batch 5: processed 25/56 chunks\n",
      "  ‚úÖ Batch 6: processed 30/56 chunks\n",
      "  ‚úÖ Batch 7: processed 35/56 chunks\n",
      "  ‚úÖ Batch 8: processed 40/56 chunks\n",
      "  ‚úÖ Batch 9: processed 45/56 chunks\n",
      "  ‚úÖ Batch 10: processed 50/56 chunks\n",
      "  ‚úÖ Batch 11: processed 55/56 chunks\n",
      "  ‚úÖ Batch 12: processed 56/56 chunks\n",
      "‚úì 56 chunks ingested successfully\n",
      "\n",
      "[9/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡∏≠‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏≠‡∏≤‡∏Å‡∏≤‡∏®.txt\n",
      "Generating embeddings for 1 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 1 chunks...\n",
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 1 chunks\n",
      "‚úì Document node created: 4604473b-4a84-46f9-84cf-3f1e7821a43b\n",
      "  ‚úÖ Batch 1: processed 1/1 chunks\n",
      "‚úì 1 chunks ingested successfully\n",
      "\n",
      "[10/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏¢‡∏≤‡∏á‡∏û‡∏§‡∏©‡∏†‡∏≤‡∏Ñ‡∏°2568.txt\n",
      "Generating embeddings for 11 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 11 chunks...\n",
      "  Processed 5/11 chunks\n",
      "  Processed 10/11 chunks\n",
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 11 chunks\n",
      "‚úì Document node created: 51f5fd68-769c-4755-8f12-b4be19e132cd\n",
      "  ‚úÖ Batch 1: processed 5/11 chunks\n",
      "  ‚úÖ Batch 2: processed 10/11 chunks\n",
      "  ‚úÖ Batch 3: processed 11/11 chunks\n",
      "‚úì 11 chunks ingested successfully\n",
      "\n",
      "[11/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡∏°‡∏±‡∏ô‡∏™‡∏≥‡∏õ‡∏∞‡∏´‡∏•‡∏±‡∏á2568_2570.txt\n",
      "Generating embeddings for 5 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 5 chunks...\n",
      "  Processed 5/5 chunks\n",
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 5 chunks\n",
      "‚úì Document node created: 856fc278-9805-49d7-a668-92bff035ca22\n",
      "  ‚úÖ Batch 1: processed 5/5 chunks\n",
      "‚úì 5 chunks ingested successfully\n",
      "\n",
      "[12/12] Processing file...\n",
      "\n",
      "üìÑ Processing: ../../resource/graph_doc\\‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°‡∏¢‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤2568_2570.txt\n",
      "Generating embeddings for 5 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings generated successfully\n",
      "Extracting section information for 5 chunks...\n",
      "  Processed 5/5 chunks\n",
      "‚úÖ Section extraction completed\n",
      "‚úì Generated 5 chunks\n",
      "‚úì Document node created: 0079b4a4-7aa5-45fb-89fd-1ea231f2487e\n",
      "  ‚úÖ Batch 1: processed 5/5 chunks\n",
      "‚úì 5 chunks ingested successfully\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "  Total files: 12\n",
      "  Successful: 12\n",
      "  Failed: 0\n",
      "\n",
      "üìà Database Statistics:\n",
      "  Total Documents: 12\n",
      "  Total Chunks: 162\n",
      "  Chunks with Embeddings: 162\n",
      "Ready to process your files!\n",
      "Update the file paths above and uncomment to use.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Example 1: Process a single file\n",
    "# success = process_single_file('../../resource/graph_doc\\‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏†‡∏π‡∏°‡∏¥‡∏≠‡∏≤‡∏Å‡∏≤‡∏®.txt', '‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏†‡∏π‡∏°‡∏¥‡∏≠‡∏≤‡∏Å‡∏≤‡∏®')\n",
    "\n",
    "# Example 2: Process all files in a folder\n",
    "process_folder('../../resource/graph_doc', file_extensions=['.txt', '.md'], max_files=12)\n",
    "\n",
    "# Example 3: Process specific files\n",
    "# files_to_process = ['/path/to/doc1.txt', '/path/to/doc2.txt']\n",
    "# for file_path in files_to_process:\n",
    "#     process_single_file(file_path)\n",
    "\n",
    "print(\"Ready to process your files!\")\n",
    "print(\"Update the file paths above and uncomment to use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
